---
title: "p8130_hw5_yl5508"
author: "Yifei LIU (yl5508)"
date: 2023/12/7
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setting, message = FALSE, warning = FALSE}
library(tidyverse)
library(faraway)
library(glmnet)
library(caret)

set.seed(123)
```

## Problem 1

**(a)** 

```{r}
#load data set
sta_data =
  as.data.frame(state.x77)

'sta_data |>
  mutate(state = rownames(state.x77))
rownames(sta_data) = NULL'

sum_sta = 
  sta_data |>
  skimr::skim() |>
  select(skim_variable, numeric.mean, numeric.sd, numeric.p0, numeric.p25, numeric.p50, numeric.p75, numeric.p100)

colnames(sum_sta) = c("Variable", "Mean", "SD", "Min", "Q1", "Median", "Q3", "Max")

knitr::kable(x = sum_sta, caption = "Variables pre-analysis", digits = 1)
```

Continuous variables includes `Population`, `Income`, `Illiteracy`, `Life Exp`, `Murder`, `HS Grad`, `Frost`, `Area`.\

Only variable `state` in the data set is categorical.\

**(b)** 

```{r}
#histogram of variables
par(mfrow = c(2, 4), mar = c(8, 4, 2, 1))

for (i in 1:8) {
  sta_data[,i] |>
  hist(main = colnames(sta_data[i]), xlab = colnames(sta_data[i]), freq = T, col = 2)
}
```

From the histograms, we notice that `Population`, `Illiteracy`, `Area` need to be transformed in order to get a normal distribution.\

```{r}
#log transformation
sta_transformed = 
  sta_data |>
  mutate(
    Population_t = log(Population),
    Illiteracy_t = log(Illiteracy),
    Area_t = log(Area)) |>
  select(Population, Population_t, Illiteracy, Illiteracy_t, Area, Area_t)

sta_tidy =
  sta_data |>
  mutate(
    Population_t = log(Population),
    Illiteracy_t = log(Illiteracy),
    Area_t = log(Area)) |>
  select(-Population, -Illiteracy, -Area) |>
  select(`Life Exp`, everything())

par(mfrow = c(3, 3), mar = c(4, 4, 2, 2))

for (i in seq(1, 5, 2)) {
  #untransformed variables
  sta_transformed[,i] |>
    hist(main = str_c("Hisogram of ", colnames(sta_transformed[i])), xlab = colnames(sta_transformed[i]), freq = T, col = 2)
  
  #log transformed variables
  sta_transformed[,i+1] |>
    hist(main = str_c("Hisogram of ", colnames(sta_transformed[i+1])), xlab = colnames(sta_transformed[i+1]), freq = T, col = 2)
  
  #Q-Q plot
  qqnorm(sta_transformed[,i+1], col = 2, pch = 19, cex = 0.5)
  qqline(sta_transformed[,i+1], col = 1, lwd = 2, lty = 2)
}
```

**(c)** 

```{r}
#global variables
lm(`Life Exp` ~ ., data = sta_tidy) |>
  summary()

#forward stepwise
model_fw = lm(`Life Exp` ~ ., data = sta_tidy) |>
  step(direction = "forward")
model_fw |> summary()

#backward stepwise
model_bk = lm(`Life Exp` ~ ., data = sta_tidy) |>
  step(direction = "backward")
model_bk |> summary()

#both
model_bth = lm(`Life Exp` ~ ., data = sta_tidy) |>
  step(direction = "both")
model_bth |> summary()
```

Based on AIC, the function reduces the set of potential predictors. The model with the smallest value would be deemed as appropriate.  

Actually the model shown after variables selection would not be the final result. We need to trim some variables off based on the p-value listed in the tables.  

For forward selection, the model shows that only variables `Murder` and `HS Grad` is significantly effective (p < 0.05).  

For backward selection, `Murder`, `HS Grad`, `Frost`, `Population_t` are significant variables.  

For method concerning both forward and backward selection, the result is the same as backward selection.  

So, I would pick `Murder`, `HS Grad`, `Frost`, `Population_t` as my predictors.  

```{r}
corrplot::corrplot(cor(sta_tidy), type = "upper")
```

We notice that there is a strong negative correlation between `Illiteracy` and `HS Grad` (approximately 0.8). My variables subset doesn't include both, for the variables selection process can partly deal with multicollinearity issue.  

**(d)** 

```{r}
library(leaps)

mat = as.matrix(sta_tidy)
leaps(x = mat[, 2:8], y = mat[, 1], method = "adjr2", nbest = 2)

model_cri = regsubsets(`Life Exp` ~ ., data = sta_tidy)
res =
  model_cri |>
  summary()

par(mfrow = c(1, 3), mar = c(8, 4, 4, 1))
plot(1:7, res$adjr2, xlab = "# of parameters", ylab = "adj r2")
plot(1:7, res$cp, xlab = "# of parameters", ylab = "Cp")
plot(1:7, res$bic, xlab = "# of parameters", ylab = "BIC")

res$outmat[4,]
```

From the criterion-based procedures, using `adjusted r squared`/`Cp criterion`/`BIC`, we conclude that the best model contain 4 parameters and the parameters are `Murder`, `HS Grad`, `Frost`, `Population_t`.

**(e)** 

```{r}
#explore possible lambda
fit1 = glmnet(x = as.matrix(sta_tidy[2:8]), y = sta_tidy$`Life Exp`, data = sta_tidy, lambda = 1)
coef(fit1)

fit2 = glmnet(x = as.matrix(sta_tidy[2:8]), y = sta_tidy$`Life Exp`, data = sta_tidy, lambda = 0.1)
coef(fit2)

fit3 = glmnet(x = as.matrix(sta_tidy[2:8]), y = sta_tidy$`Life Exp`, data = sta_tidy, lambda = 0.01)
coef(fit3)
```

We would consider setting the range of lambda at the interval of (0.01, 0.1).  

```{r}
#grid search
lambda_seq = 10^seq(-2, 1, by = 0.1)
```


```{r warning = FALSE}
cv_res =
  cv.glmnet(x = as.matrix(sta_tidy[2:8]), y = sta_tidy$`Life Exp`, data = sta_tidy, 
            lambda = lambda_seq, nfolds = 5)

opt_lambda = cv_res$lambda.min

#variables contraction
glmnet(x = as.matrix(sta_tidy[2:8]), y = sta_tidy$`Life Exp`, data = sta_tidy, lambda = lambda_seq) |>
  broom::tidy() |>
  select(term, lambda, estimate) |> 
  complete(term, lambda, fill = list(estimate = 0) ) |> 
  filter(term != "(Intercept)") |> 
  ggplot(aes(x = log(lambda, 10), y = estimate, group = term, color = term)) + 
  geom_path(size = 0.8) + 
  geom_vline(xintercept = log(opt_lambda, 10), color = "red", linetype = "dashed", size = 1) +
  theme_bw() +
  theme(legend.position = "bottom")

tb_res = tibble(
  lambda = cv_res$lambda,
  mean_cv_error = cv_res$cvm) |>
  filter(lambda < 0.1)

#choosing optimal lambda
tb_res |>
  ggplot(aes(x = lambda, y = mean_cv_error)) +
  geom_point() +
  theme_bw()
```

The optimal lambda we have chosen is `r round(opt_lambda, 2)`. And the variables we determine are `Murder`, `HS Grad`, `Frost`, `Population_t`.  

**(f)** 

```{r}
#stepwise
fit_bth = lm(`Life Exp` ~ ., data = sta_tidy) |>
  step(direction = "both")

summary(fit_bth)$adj.r.squared
```

From above exploring, we find that all models or criteria suggest a variables combination of `Murder`, `HS Grad`, `Frost`, `Population_t`. So, I recommend final model to be a multiple linear model of `Life Exp` ~ `Murder` + `HS Grad` + `Frost` + `Population_t` (r squared = `r round(summary(fit_bth)$adj.r.squared, 2)`).  
*(1) Check the model assumptions.*

```{r}
#checking assumptions
par(mfrow = c(2, 2))
plot(fit_bth)
```

For `Residuals vs Fitted plot`, it is used to detect unequal error variance (heteroscedasticity) and outliers. Basically, the plot shows residual values bounce around 0. Except `Hawaii`, they are in the range of (-1.5, 1.5) indicating only `Hawaii` could be a potential outlier.  

For `Q-Q plot`, it is used to detect non-normality of residuals and outliers. The plot shows a straight line indicating residuals are normal. `Hawaii` is an extreme point to some degree.  

For `Scale-Location plot` (`Standardized Residuals ~ Fitted plot`), it is used to detect unequal error variance (heteroscedasticity). The variances are equal.  

For `Residuals vs Leverage plot`, it is used to detect influential cases. We notice that `Hawaii` is just on the Cook's distance line at the upper right corner. `Hawaii` could be an influential case.  

Overall, assumptions of regression are met.  

```{r}
#delete extreme outlier `Hawaii`
fit_bth_de = lm(`Life Exp` ~ ., data = sta_tidy[- 11, ]) |>
  step(direction = "both")

tibble(
  original_fit = summary(fit_bth)$adj.r.squared,
  deletion_fit = summary(fit_bth_de)$adj.r.squared
) |>
  knitr::kable(digits = 2)
```

*(2) Test the model predicative ability using a 10-fold cross-validation.*

```{r}
#cross validation
train = trainControl(method = "cv", number = 5)

model_cv = train(`Life Exp` ~ Murder + `HS Grad` + Frost + Population_t, data = sta_tidy[- 11, ], method = 'lm', na.action = na.pass)

model_cv$finalModel

print(model_cv)
summary(model_cv)$adj.r.squared
```

R squared of cross validation is `r round(summary(model_cv)$adj.r.squared, 2)`, suggesting the model fits quite well and it explains a great proportion of outcome `Life Exp`.  

**(g)** 

From analysis concerning with relevant data set, we find out several indicators which would predict a large proportion of the target outcome.  

In pre-processing procedure, we use log function to transform some variables which do not follow a normal distribution.  

By using automatic variables selection procedure (stepwise), criterion-based procedure and LASSO, we get the best subset of the variables, which contains `Murder`, `HS Grad`, `Frost`, `Population_t`. These variables are with the highest adjusted r squared, lowest Cp and BIC.  

After generating our final model, we check the basic assumptions. We notice that assumption of equal variance is met and assumption of residual normality is met. But one extreme case (`Hawaii`) is detected and it has been removed in later analysis.  

Lastly, a 10-fold cross-validation is taken into account and the model is shown fitting quite well according to the adjusted r squared result. This suggests that the final model can successfully predict life expectancy among states as the investigator requests.  




